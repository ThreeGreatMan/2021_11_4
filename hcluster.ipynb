{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是我电脑自己环境的问题，你们不用运行\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.remove('C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "from math import *\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "colors = [\"grey\", \"brown\", \"orange\", \"olive\", \"green\", \"cyan\",\n",
    "              \"blue\", \"purple\", \"pink\", \"red\"]\n",
    "\n",
    "# colors = ['black', 'gray', 'firebrick', 'red', 'chocolate',\n",
    "#               'orange', 'peru', 'darkgoldenrod', 'olive',\n",
    "#               'yellow', 'green', 'turquoise', 'deepskyblue',\n",
    "#               'blue', 'violet', 'purple', 'deeppink']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. LDA, DSC, dDSC, GONG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回质心向量\n",
    "def getTwoClassMean(c0, c1):\n",
    "    \"\"\"\n",
    "    get the mean vector of two class\n",
    "    \"\"\"\n",
    "    mean0_vectors = np.mean(c0, axis=0)\n",
    "    mean1_vectors = np.mean(c1, axis=0)\n",
    "    return mean0_vectors, mean1_vectors\n",
    "# 返回SW SB\n",
    "def getClassScatterMatrix(c0, c1):\n",
    "    \"\"\"\n",
    "    input two class data\n",
    "    return the scatter for each class\n",
    "    and with-in sc_mat and between sc_mat\n",
    "    \"\"\"\n",
    "    mean0_vectors, mean1_vectors = getTwoClassMean(c0, c1)\n",
    "    num0 = c0.shape[0]\n",
    "    num1 = c1.shape[0]\n",
    "    dim = c0.shape[1]\n",
    "    class0_sc_mat = np.zeros((dim, dim))\n",
    "    mv = mean0_vectors.reshape((dim, 1))\n",
    "    for row in c0:\n",
    "        row = row.reshape((dim, 1))\n",
    "        class0_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    class1_sc_mat = np.zeros((dim, dim))\n",
    "    mv = mean1_vectors.reshape((dim, 1))\n",
    "    for row in c1:\n",
    "        row = row.reshape((dim, 1))\n",
    "        class1_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    S_W = class0_sc_mat + class1_sc_mat\n",
    "    c = np.concatenate((c0, c1))  # Vertical stacking, default axis is 0\n",
    "    overall_mean = np.mean(c, axis=0)\n",
    "    overall_mean = overall_mean.reshape((dim, 1))\n",
    "    mean_vec = mean0_vectors.reshape((dim, 1))\n",
    "    S_B = np.zeros((dim, dim))\n",
    "    S_B += num0 * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "    mean_vec = mean1_vectors.reshape((dim, 1))\n",
    "    S_B += num1 * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "    return class0_sc_mat, class1_sc_mat, S_W, S_B\n",
    "\n",
    "\n",
    "# 返回LDA目标函数在该投影下的值\n",
    "def getLDAValue(S_W, S_B, w):\n",
    "    result_cost = 1.0 * w.T.dot(S_B).dot(w) / w.T.dot(S_W).dot(w)\n",
    "    return result_cost\n",
    "\n",
    "\n",
    "# 手动实现 LDA\n",
    "def lda(c0, c1):\n",
    "    \"\"\"\n",
    "    type is ndarray\n",
    "    shape like (200,2) and 200 is the number of samples and 2 is the number of features\n",
    "    pass two class data to the function\n",
    "    return the Eigenvectors using LDA\n",
    "    the solve method is Eigenvalue Decomposition\n",
    "    \"\"\"\n",
    "    # get the basic information\n",
    "    dim = c0.shape[1]  # the dim is the number of features, for instance, the dim of iris is 4\n",
    "    c = np.concatenate((c0, c1))  # Vertical stacking, default axis is 0\n",
    "    overall_mean = np.mean(c, axis=0)  # the overall_mean of the samples\n",
    "    # print(\"overall_mean: \", overall_mean)\n",
    "    num0 = c0.shape[0]  # the number of class-0\n",
    "    num1 = c1.shape[0]  # the number of class-1\n",
    "\n",
    "    # Computing the mean vectors\n",
    "    mean0_vectors = np.mean(c0, axis=0)\n",
    "    mean1_vectors = np.mean(c1, axis=0)\n",
    "    S_W = np.zeros((dim, dim))\n",
    "    class0_sc_mat = np.zeros((dim, dim))\n",
    "    mv = mean0_vectors.reshape(dim, 1)\n",
    "    # print(\"mv: \", mv)\n",
    "    for row in c0:\n",
    "        row = row.reshape(dim, 1)\n",
    "        # print(\"row-mv: \\n\", row-mv)\n",
    "        class0_sc_mat += (row-mv).dot((row-mv).T)\n",
    "        # print(\"class0_sc_mat: \", class0_sc_mat)\n",
    "    S_W += class0_sc_mat\n",
    "    # print(\"within-class Scatter Matrix: \\n\", S_W)\n",
    "\n",
    "    # for class1, al S_W1\n",
    "    class1_sc_mat = np.zeros((dim, dim))\n",
    "    mv = mean1_vectors.reshape((dim, 1))\n",
    "    # print(\"mv: \", mv)\n",
    "    for row in c1:\n",
    "        row = row.reshape(dim, 1)\n",
    "        class1_sc_mat += (row-mv).dot((row-mv).T)\n",
    "        \n",
    "    \n",
    "    S_W += class1_sc_mat\n",
    "    # print(\"within-class Scatter Matrix: \\n\", S_W)\n",
    "\n",
    "    # Between-class scatter matrix S_B\n",
    "    S_B = np.zeros((dim, dim))\n",
    "    # print(\"overall_mean: \", overall_mean)\n",
    "    overall_mean = overall_mean.reshape(dim, 1)\n",
    "    mean_vec = mean0_vectors.reshape(dim, 1)\n",
    "    S_B += num0*(mean_vec-overall_mean).dot((mean_vec-overall_mean).T)\n",
    "    mean_vec = mean1_vectors.reshape(dim, 1)\n",
    "    S_B += num1*(mean_vec-overall_mean).dot((mean_vec-overall_mean).T)\n",
    "    # print(\"between-class Scatter Matrix: \\n\", S_B)\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.pinv(S_W).dot(S_B))\n",
    "    # for i in range(len(eig_vals)):\n",
    "    #     eigvec_sc = eig_vecs[:, i].reshape(dim, 1)\n",
    "    #     print(\"\\nEigenvector {}: \\n{}\".format(i+1, eigvec_sc.real))\n",
    "    #     print(\"Eigenvalue {:}: {:.2e}\".format(i+1, eig_vals[i].real))\n",
    "\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
    "\n",
    "\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "\n",
    "    return [i[1] for i in eig_pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_to_point(P1, P2):\n",
    "\n",
    "    dx = abs(P1[0] - P2[0])\n",
    "    dy = abs(P1[1] - P2[1])\n",
    "    return math.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "\n",
    "# 计算一个点的GONG值，即所定义的set中与自己同类别所占的比例\n",
    "def cal_one_point_GONG(Points, Label, i):\n",
    "    omega = 0\n",
    "    same_indicator = 0\n",
    "    tot_num = Points.shape[0]\n",
    "    for j in range(tot_num):\n",
    "        if j == i:\n",
    "            continue\n",
    "        flag = True\n",
    "        intermediary_point = 0.35 * Points[j] + 0.65 * Points[i]\n",
    "        int2j_dist = get_distance_to_point(intermediary_point, Points[j])\n",
    "        for k in range(tot_num):\n",
    "            if k == i:\n",
    "                continue\n",
    "            dist = get_distance_to_point(intermediary_point, Points[k])\n",
    "            if dist < int2j_dist:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag == False:\n",
    "            continue\n",
    "        omega += 1\n",
    "        if Label[i] == Label[j]:\n",
    "            same_indicator += 1\n",
    "    if omega == 0:\n",
    "        return 0\n",
    "    return same_indicator/omega\n",
    "\n",
    "\n",
    "# 计算某个类的GONG值（该类下所有点的GONG值取平均）\n",
    "def get_label_GONG(Points, Label, lab):\n",
    "    GONG_list = []\n",
    "    tot_num = Points.shape[0]\n",
    "    for i in range(tot_num):\n",
    "        GONG_list.append(cal_one_point_GONG(Points, Label, i))\n",
    "    GONG_list = np.array(GONG_list)\n",
    "    return np.mean(GONG_list[Label == lab])\n",
    "\n",
    "\n",
    "# 计算一个投影的GONG（即每个类的GONG值的加权平均）\n",
    "def get_GONG_value(data, Label, evaluation):\n",
    "    if evaluation == \"Silhouette Coefficient\":\n",
    "        return metrics.silhouette_score(Points, Label)\n",
    "    answer = 0\n",
    "    for lab in set(label):\n",
    "        answer += get_label_GONG(data, label, lab) * len(label[label == lab]) / len(label)\n",
    "    return answer\n",
    "    \n",
    "    \n",
    "def get_weighted_GONG(data, label, proj_set, proj_set_labels):\n",
    "    \"\"\"\n",
    "    计算整个图的加权GONG值\n",
    "    输入：原始数据data，label，投影集projection_set\n",
    "    data和一个投影平面projection_set[i],可以组成投影后的data_\n",
    "    对于每个类：\n",
    "    计算这个类在所有投影中的GONG\n",
    "    取最大值，作为这个类的GONG\n",
    "    最终对每个类的GONG加权，得到整个图的加权GONG\n",
    "    \"\"\"\n",
    "    proj_num = len(proj_set)\n",
    "    \n",
    "    label_count = {}\n",
    "    GONG = []# GONG[i][j] 代表：在投影i上，第j个类的GONG值\n",
    "    for i in range(proj_num):\n",
    "        proj = proj_set[i]\n",
    "        proj_label = proj_set_labels[i]\n",
    "        GONG_list = {}\n",
    "\n",
    "        data_0, data_1, data_, label_ = project_on_merged_vectors(data, label, set(proj_label), proj[0], proj[1])\n",
    "        \n",
    "        # 计算该投影下每一个类的GONG\n",
    "        for lab in set(label_): \n",
    "            GONG_list[lab] = get_label_GONG(data_,label_,lab)\n",
    "            label_count[lab] = len(label[label==lab])\n",
    "            \n",
    "        print(GONG_list)\n",
    "        # 存入GONG\n",
    "        GONG.append(GONG_list)\n",
    "        \n",
    "    label_GONG = {}\n",
    "    for GONG_list in GONG:\n",
    "        for key in GONG_list:\n",
    "            if key not in label_GONG:\n",
    "                label_GONG[key] = GONG_list[key]\n",
    "            else:\n",
    "                label_GONG[key] = max(label_GONG[key], GONG_list[key])\n",
    "            \n",
    "    print('-------')\n",
    "    print(label_GONG)\n",
    "    print(label_count)\n",
    "    weighted_GONG = 0 \n",
    "    \n",
    "    cnt = 0\n",
    "    for key in label_GONG:\n",
    "        weighted_GONG += label_GONG[key] * label_count[key]\n",
    "        cnt += label_count[key]\n",
    "\n",
    "    weighted_GONG /= cnt\n",
    "    return weighted_GONG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 3D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data_sets(n_samples=50, centers=9, n_features=3, mean_step = 5, cov_step = 120):\n",
    "\n",
    "    newX = np.array([])\n",
    "    for i in range(centers):\n",
    "        mean = np.random.randint(40, 100, size=n_features)\n",
    "        mat = np.random.randint(10, 22, (n_features, n_features))\n",
    "        mean += mean_step\n",
    "        mat += cov_step\n",
    "        cov = np.cov(mat)\n",
    "        cov = np.ceil(cov)\n",
    "        cov = abs(cov)\n",
    "        new_x = np.random.multivariate_normal(mean, cov, (n_samples), 'raise')\n",
    "        # print(\"newX\",newX)\n",
    "        # print(\"new_x\",new_x)\n",
    "        newX = list(newX)\n",
    "        new_x = list(new_x)\n",
    "        newX.extend(new_x)\n",
    "        newX = np.array(newX)\n",
    "    print(newX.shape)\n",
    "\n",
    "    y = []\n",
    "    for i in range(centers):\n",
    "        for j in range(n_samples):\n",
    "            y.append(i)\n",
    "    y = np.array(y)\n",
    "    return newX,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = generate_data_sets(50,9,2,0,0)\n",
    "plt.scatter(x1[:,0],x1[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X, y = generate_data_sets(n_samples=50, centers=5, n_features=6)\n",
    "\n",
    "\n",
    "# ax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程\n",
    "# #  将数据点分成三部分画，在颜色上有区分度\n",
    "# ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y)  # 绘制数据点\n",
    "\n",
    "# ax.set_zlabel('Z')  # 坐标轴\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_xlabel('X')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    :param filename: the data file, the format is class_label, dim0, dim1, ...\n",
    "    :return: data and label\n",
    "    \"\"\"\n",
    "    newfilename = './Data/' + filename  # because my data is in the Data folder\n",
    "    data = []\n",
    "    with open(newfilename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(list(eval(line)))\n",
    "    data = np.array(data)\n",
    "    dim = data.shape[1]\n",
    "    return data[:, 1:dim], data[:, 0]\n",
    "\n",
    "\n",
    "# # 重新编排label为0, 1, 2,...\n",
    "def get_label_right(label):\n",
    "    class_label = list(set(label))\n",
    "    label_dict = {}\n",
    "    re_label = 0\n",
    "    for it in class_label:\n",
    "        label_dict[it] = re_label\n",
    "        re_label += 1\n",
    "    new_label = []\n",
    "    for it in label:\n",
    "        new_label.append(label_dict[it])\n",
    "    return np.array(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2个类的文件都要踢出来\n",
    "path = r'.\\Data'\n",
    "dirs = os.listdir(path)\n",
    "for fname in dirs:\n",
    "    data, label = read_data_from_file(fname)\n",
    "    # if(len(set(label)) == 2):\n",
    "    print(fname, len(set(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following two lines to fit your data\n",
    "fname = \"world_9d.csv\"  # the class data file\n",
    "data, label = read_data_from_file(fname)\n",
    "# data, label = X, y\n",
    "label = get_label_right(label)\n",
    "label = np.array(list(map(int, label)))\n",
    "print(data.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?\n",
    "# 确保该向量方向是对的\n",
    "\n",
    "def get_right_direction(vec, last_axis):\n",
    "    if np.dot(vec, last_axis) < 0:\n",
    "        return -vec\n",
    "    return vec\n",
    "\n",
    "\n",
    "def normalize(v):\n",
    "        return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "# 第一步：寻找LDA下的最佳discriminative vector\n",
    "def get_optimal_vector_from_lda(c0, c1):\n",
    "    discriminating_vectors = lda(c0, c1)\n",
    "    opt_vec = np.real(discriminating_vectors[0])  # ndarray\n",
    "    return opt_vec\n",
    "\n",
    "\n",
    "# ? last_axis\n",
    "# 寻找每两个类之间的最佳投影向量  (c0,c1):vector\n",
    "def get_between_class_opt_vec(data, label):\n",
    "    \"\"\"\n",
    "    :param data: the input data\n",
    "    :param label: The label of the data\n",
    "    :return: the optimal discriminant vector corresponding to (class a, class b)\n",
    "    \"\"\"\n",
    "    class_list = list(set(label))\n",
    "    vec_dict = {}\n",
    "    class_num = len(class_list)\n",
    "    # dim = data.shape[1]\n",
    "    # last_axis = np.array([0 for i in range(dim)])\n",
    "    # last_axis[dim - 1] = last_dim\n",
    "    # 对于每两个类\n",
    "    for i in range(class_num):\n",
    "        for j in range(i+1, class_num):\n",
    "            # 计算最佳投影向量并加入字典\n",
    "            opt_vec = get_optimal_vector_from_lda(data[label == class_list[i]],data[label == class_list[j]])\n",
    "            opt_vec = normalize(opt_vec)\n",
    "            # opt_vec = get_right_direction(opt_vec, last_axis)\n",
    "            vec_dict[(class_list[i], class_list[j])] = opt_vec\n",
    "    vecs = []\n",
    "    for key in vec_dict:\n",
    "        vecs.append(np.array(vec_dict[key]))\n",
    "    vecs = np.array(vecs)\n",
    "    return vec_dict, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict, vecs = get_between_class_opt_vec(data, label)\n",
    "display(vecs)      # vec_dict.values() 按指定顺序\n",
    "display(vec_dict)  # key: (vid1, vid2), value: discriminative vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_pair(vec_dict):\n",
    "    i = 0\n",
    "    id_pair = {}\n",
    "    id_vecs = {}\n",
    "    for key in vec_dict:\n",
    "        id_pair[i] = key\n",
    "        id_vecs[i] = vec_dict[key]\n",
    "        i += 1\n",
    "    return id_pair, id_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid -- labels -- vec 三者一一对应\n",
    "# vid号判别向量区分pair对应的两个label，这个判别向量存在vec内\n",
    "id_pair, id_vecs = get_id_pair(vec_dict)\n",
    "print(id_vecs)\n",
    "print(id_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos(angle) = sqrt(<v11, v12>^2 + <v21, v22>^2)/2\n",
    "cos 值越大，angle越小，两平面越\"相似\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corresponding_color(id):\n",
    "    return colors[id]\n",
    "\n",
    "def cal_angle(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Problem1：平面角度的计算\n",
    "def get_distance(plane1, plane2):\n",
    "    n = len(plane1)\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res += cal_angle(plane1[i], plane2[i])**2\n",
    "    res /= n \n",
    "    return np.sqrt(res)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# # 利用协方差最大的方向进行降维\n",
    "# # 数据集: 降维后的维度\n",
    "# # return: 拟合平面的两个基向量组成的list\n",
    "# def PCA_fitting_plane(dataMat, topNfeat=2):\n",
    "#     dataMat = np.mat(dataMat)\n",
    "\n",
    "#     meanVals = np.mean(dataMat, axis=0)\n",
    "#     meanRemoved = dataMat - meanVals\n",
    "#     covMat = np.cov(meanRemoved, rowvar=0)  # 得到协方差矩阵\n",
    "\n",
    "#     # 得到基于协方差矩阵的特征值和特征向量（一列表示一个特征向量）\n",
    "#     eigVals, eigVects = np.linalg.eig(np.mat(covMat))\n",
    "#     eigValInd = np.argsort(eigVals)  # 对特征值进行排序（从小到大），返回其对应的数组索引（按从小到大顺序）\n",
    "#     # 从排序后的矩阵最后一个开始自下而上选取最大的N个特征值，返回其对应的数组索引\n",
    "#     eigValInd = eigValInd[:-(topNfeat + 1):-1]\n",
    "#     redEigVects = eigVects[:, eigValInd]  # 从特征向量数组中取出特征值较大的特征向量组成数组\n",
    "#     # lowDDataMat = meanRemoved * redEigVects  # 将去除均值后的数据矩阵*压缩矩阵，转换到新的空间，使维度降低为N\n",
    "#     # 返回压缩后的数据矩阵即该矩阵反构出原始数据矩阵\n",
    "#     return redEigVects\n",
    "\n",
    "\n",
    "# 使用前提：plain 的基坐标 v1，v2 是单位向量\n",
    "def get_dist_to_plain(plain, v):\n",
    "    length = int(len(plain)/2)\n",
    "    v1, v2 = plain[0:length], plain[length:]\n",
    "    new_v = np.dot(np.dot(v1, v), v1) + np.dot(np.dot(v2, v), v2)\n",
    "    return np.linalg.norm(v-new_v)\n",
    "    \n",
    "    \n",
    "def get_fitted_plain(vecs):\n",
    "    if len(vecs) == 2:\n",
    "        axis1, axis2 = vecs[0], vecs[1]\n",
    "\n",
    "        axis1 = normalize(axis1)\n",
    "        a = 1\n",
    "        axis1_length = 1\n",
    "        b = -a / np.dot(axis2, axis1)\n",
    "        axis2 = normalize(a*axis1+b*axis2)\n",
    "        plain = np.array([axis1, axis2])\n",
    "        print('dists-->',np.sum([get_dist_to_plain(plain, feature) for feature in vecs]))\n",
    "        return np.array([axis1, axis2])\n",
    "\n",
    "    length = len(vecs[0])\n",
    "\n",
    "    def fun(args):\n",
    "        _vecs = args\n",
    "        dists = lambda P: np.sum([get_dist_to_plain(P, v) for v in _vecs])\n",
    "        return dists\n",
    "\n",
    "    def con():\n",
    "        # 约束条件：两个基模长为1\n",
    "        cons = (\n",
    "            {'type': 'eq', 'fun': lambda P: np.linalg.norm(P[0])-1},\n",
    "            {'type': 'eq', 'fun': lambda P: np.linalg.norm(P[1])-1}\n",
    "        )\n",
    "        return cons\n",
    "\n",
    "    # Problem2: 条件极值？\n",
    "    # ans = fmin(fun, np.array([normalize(np.random.random(length)), normalize(\n",
    "    #     np.random.random(length))]), maxfun=100000)\n",
    "\n",
    "    cons = con()\n",
    "    ans = minimize(fun(vecs), x0=np.array([normalize(np.random.random(length)), normalize(\n",
    "          np.random.random(length))]), method='BFGS', constraints=cons, options={\"disp\": True})\n",
    "\n",
    "    print(ans.x)\n",
    "    print(ans.fun)\n",
    "    ans_x = ans.x\n",
    "\n",
    "    axis1, axis2 = ans_x[0:length], ans_x[length:]\n",
    "    print(np.linalg.norm(axis1), np.linalg.norm(axis2))\n",
    "    plain = np.array([axis1, axis2])\n",
    "\n",
    "    axis1 = normalize(axis1)\n",
    "    a = 1\n",
    "    b = -a / np.dot(axis2, axis1)\n",
    "    axis2 = normalize(a*axis1+b*axis2)\n",
    "\n",
    "    plain = np.array([axis1, axis2])\n",
    "    print('dists-->',\n",
    "          np.sum([get_dist_to_plain(plain, feature) for feature in vecs]))\n",
    "    return plain\n",
    "\n",
    "# def get_all_planes(vecs, id_pair):\n",
    "#     planes = []\n",
    "#     pid_planes = {}\n",
    "#     pid_labels = {}\n",
    "#     pid = 0\n",
    "#     for i in range(len(vecs)):\n",
    "#         for j in range(i + 1, len(vecs)):\n",
    "#             plane = get_fitted_plain([vecs[i], vecs[j]])\n",
    "#             planes.append(plane)\n",
    "#             pid_labels[pid] = list(set([id_pair[i][0], id_pair[i][1], id_pair[j][0], id_pair[j][1]]))\n",
    "#             pid_planes[pid] = [vecs[i], vecs[j]]\n",
    "#             pid += 1\n",
    "    \n",
    "#     return planes, pid_labels, pid_planes\n",
    "\n",
    "\n",
    "def get_all_planes(vecs, id_pair):\n",
    "    planes = []\n",
    "    pid_vids = {}\n",
    "    pid_labels = {}\n",
    "    pid = 0\n",
    "    for i in range(len(vecs)):\n",
    "        for j in range(i + 1, len(vecs)):\n",
    "            plane = get_fitted_plain([vecs[i], vecs[j]])\n",
    "            print(plane)\n",
    "            planes.append(plane)\n",
    "            pid_labels[pid] = list(set([id_pair[i][0], id_pair[i][1], id_pair[j][0], id_pair[j][1]]))\n",
    "            pid_vids[pid] = [i, j]\n",
    "            pid += 1\n",
    "    \n",
    "    return planes, pid_labels, pid_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes, pid_labels, pid_vids = get_all_planes(vecs, id_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hierarchical:\n",
    "    def __init__(self, center, left=None, right=None, vec_ids=None, flag=None, distance=0.0):\n",
    "        self.center = center # 记录的是 plane (axis1, axis2) 保证正交\n",
    "        self.left = left\n",
    "        \n",
    "        self.right = right\n",
    "        self.vec_ids = vec_ids\n",
    "        self.flag = flag\n",
    "        self.distance = distance\n",
    "\n",
    "\n",
    "def traverse_id(node):\n",
    "    if node.left == None and node.right == None:\n",
    "        return [node.flag]\n",
    "    else:\n",
    "        return traverse_id(node.left)+traverse_id(node.right)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# data=[plane1, plane2, ..., plane3]\n",
    "# plane =[v1id, v2id]\n",
    "\n",
    "# clusters = [clu1, clu2, ...]\n",
    "# clu = [vec1id, vec2id, ...]\n",
    "# center = [vec1, vec2]\n",
    "\n",
    "\n",
    "# 思路\n",
    "# 一个节点记录了 plain 以及组成该 plain 的 vec\n",
    "# 每次计算两两 plain 之间的距离得到距离矩阵，从其中挑出距离最小的两个 plain 对\n",
    "# 这两个 plain 对应一堆 vec（分别组成每一个平面）\n",
    "# 现在最优化目标是：寻找一个平面，使这一堆 vec 到这个平面的距离最小（注意时刻保持拟合平面基坐标正交以及）\n",
    "# 找到这样的平面后，我们会得到两个基向量\n",
    "\n",
    "# 最优化目标能否改为：拟合一个平面，使其到各个平面的夹角之和最小\n",
    "\n",
    "\n",
    "def hcluster(data, pid_vids, id_vecs, n):\n",
    "    if len(data) <= 0:\n",
    "        print('invalid data')\n",
    "    # 初始化每一点为一个簇\n",
    "    clusters = [Hierarchical(center=data[i], vec_ids=pid_vids[i], flag=i) for i in range(len(data))]\n",
    "    # 距离字典\n",
    "    distances = {}\n",
    "    min_id1 = None\n",
    "    min_id2 = None\n",
    "    current_cluster = -1\n",
    "    maxv = int(1e9)\n",
    "\n",
    "    while len(clusters) > n:\n",
    "        min_dist = maxv\n",
    "        \n",
    "        for i in range(len(clusters)-1):\n",
    "            for j in range(i+1, len(clusters)):\n",
    "                # 计算两两簇之间的距离\n",
    "                if distances.get((clusters[i], clusters[j])) == None:\n",
    "                    distances[(clusters[i].flag, clusters[j].flag)] = get_distance(\n",
    "                        clusters[i].center, clusters[j].center)\n",
    "\n",
    "                # 如果距离小于目前遇到的最小距离\n",
    "                # 记录这两个簇ID以及最小距离\n",
    "                if distances[(clusters[i].flag, clusters[j].flag)] <= min_dist:\n",
    "                    min_id1 = i\n",
    "                    min_id2 = j\n",
    "                    min_dist = distances[(clusters[i].flag, clusters[j].flag)]\n",
    "\n",
    "        # 如果存在最小距离\n",
    "        if min_id1 != None and min_id2 != None and min_dist != maxv:\n",
    "            # 则把这两个ID对应的簇合并\n",
    "            vec_ids = [v for v in clusters[min_id1].vec_ids] + [v for v in clusters[min_id2].vec_ids]\n",
    "            vec_ids = list(set(vec_ids))\n",
    "            print(vec_ids)\n",
    "            vecs = [id_vecs[vid] for vid in vec_ids]\n",
    "            new_center = get_fitted_plain(vecs)\n",
    "            new_flag = current_cluster\n",
    "            current_cluster -= 1\n",
    "            new_cluster = Hierarchical(\n",
    "                new_center, clusters[min_id1], clusters[min_id2], vec_ids, new_flag, min_dist)\n",
    "            del clusters[min_id2]\n",
    "            del clusters[min_id1]\n",
    "            clusters.append(new_cluster)\n",
    "        print('-'*50)\n",
    "        # print(distances)\n",
    "\n",
    "    final_cluster = [traverse_id(clusters[i]) for i in range(len(clusters))]\n",
    "    return final_cluster\n",
    "\n",
    "\n",
    "def get_cluster(planes, pid_vids, id_vecs, n):\n",
    "    clusters = hcluster(planes, pid_vids, id_vecs, n)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def get_proj_set_and_label(clusters, id_vecs, pid_labels, pid_vids):\n",
    "    proj_labels = []\n",
    "    proj_set = []\n",
    "    for clu in clusters:\n",
    "        ids = clu\n",
    "        labels = [pid_labels[i][j] for i in ids for j in range(len(pid_labels[i]))]\n",
    "        proj_labels.append(np.array(list(set(labels))))\n",
    "\n",
    "        vids = list(set([pid_vids[i][j] for i in ids for j in range(len(pid_vids[i]))]))\n",
    "        print('vids: ', vids)\n",
    "        vecs = [id_vecs[i] for i in vids]\n",
    "        proj_set.append(get_fitted_plain(vecs))\n",
    "    return proj_labels, proj_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = get_cluster(planes, pid_vids, id_vecs, n=4)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_labels, proj_set = get_proj_set_and_label(clusters, id_vecs, pid_labels, pid_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_merged_vectors(data, label, plot_class_set, axis1, axis2, ax=None):\n",
    "    '''\n",
    "        axis?: 一个向量集所对应的box区域的交集部分的中心向量\n",
    "        set? : axis?所对应的向量集所能区分的labels\n",
    "    '''\n",
    "    data_0 = [] # 存储各个点投影到 2D 平面后的横坐标\n",
    "    data_1 = [] # 存储各个点投影到 2D 平面后的纵坐标\n",
    "    label_ = [] # 各个点的label\n",
    "    for it in set(plot_class_set):\n",
    "        x = np.dot(data[label == it], axis1)\n",
    "        if it in plot_class_set:\n",
    "            data_0.extend(list(x))\n",
    "        \n",
    "        if len(axis2):\n",
    "            y = np.dot(data[label == it], axis2)\n",
    "            if it in plot_class_set:\n",
    "                data_1.extend(list(y))\n",
    "            \n",
    "        if it in plot_class_set:\n",
    "            label_.extend([it]*len(x))\n",
    "        \n",
    "        if ax:\n",
    "            if it in plot_class_set:\n",
    "                ax.scatter(x, y, color=get_corresponding_color(it), label=str(it), alpha=1, s=10)\n",
    "            else:\n",
    "                ax.scatter(x, y, color=get_corresponding_color(it), label=str(it), alpha=0.1, s=10)\n",
    "                \n",
    "    if ax:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(0.2, 1.12),ncol=3)\n",
    "    \n",
    "    if len(axis2) == 0:\n",
    "        data_1 = [0 for _ in range(len(data_0))]\n",
    "        \n",
    "    data_ = np.vstack([data_0,data_1]).T\n",
    "    label_ = np.array(label_)\n",
    "    return data_0, data_1, data_, label_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_projection(proj_set, data, label, vecs_label):\n",
    "    plot_tot = int(len(proj_set))\n",
    "\n",
    "    # 投影后的点的矩阵的shape\n",
    "    row_num = int(np.ceil(plot_tot / 2.0))\n",
    "    col_num = 2     # 列数=总图数/行数\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=row_num, ncols=col_num, squeeze=False, figsize=(10, 10))\n",
    "\n",
    "    pos = 0 \n",
    "\n",
    "    # 对每一个图\n",
    "    for i in range(row_num):\n",
    "        for j in range(col_num):\n",
    "            proj = proj_set[pos]\n",
    "            sep_labels = [i for i in vecs_label[pos]]\n",
    "            print(sep_labels)\n",
    "            project_on_merged_vectors(data, label, sep_labels, proj[0], proj[1], ax[i][j])\n",
    "            pos += 1\n",
    "            if pos == plot_tot:\n",
    "                break\n",
    "        if pos == plot_tot:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_projection(proj_set, data, label, proj_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weighted_GONG(data, label, proj_set, proj_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lda(data, label):\n",
    "    clf = LDA(n_components=2)\n",
    "    new_data_LDA = clf.fit_transform(data, label)\n",
    "    plt.scatter(new_data_LDA[:, 0], new_data_LDA[:, 1], c=label,\n",
    "            cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "    GONG = get_GONG_value(new_data_LDA, label, 'test')\n",
    "    plt.title('LDA: ' + str(round(GONG, 4)))\n",
    "    \n",
    "#     plt.savefig('./result/' + fname + '/' + fname.split('.')[0] + '_LDA.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return GONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pca(data, label):\n",
    "    pca = PCA(n_components=2)\n",
    "    new_data_PCA = pca.fit_transform(data)\n",
    "\n",
    "    plt.scatter(new_data_PCA[:, 0], new_data_PCA[:, 1], c=label,\n",
    "            cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "    GONG = get_GONG_value(new_data_PCA, label, 'test')\n",
    "    plt.title('PCA: ' + str(round(GONG, 4)))\n",
    "    \n",
    "#     plt.savefig('./result/' + fname + '/' + fname.split('.')[0] + '_PCA.png')\n",
    "\n",
    "    plt.show()\n",
    "    return GONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tsne(data, label):\n",
    "    sc = StandardScaler()\n",
    "    data_std = sc.fit_transform(data)\n",
    "\n",
    "    tsne = TSNE(n_components=2, learning_rate=100)\n",
    "    new_data_tsne = tsne.fit_transform(data)\n",
    "\n",
    "    plt.scatter(new_data_tsne[:, 0], new_data_tsne[:, 1], c=label,\n",
    "            cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "    GONG = get_GONG_value(new_data_tsne, label, 'test')\n",
    "    plt.title('t-SNE: ' + str(round(GONG, 4)))\n",
    "    \n",
    "#     plt.savefig('./result/' + fname + '/' + fname.split('.')[0] + '_TSNE.png')\n",
    "\n",
    "    plt.show()\n",
    "    return GONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kda(data, label):\n",
    "    kda = KernelDiscriminantAnalysis()\n",
    "    a = kda.fit(data, label)\n",
    "    b = kda.transform(data)\n",
    "    new_data_KDA = kda.fit_transform(data,label)\n",
    "    plt.scatter(new_data_KDA[:,0],new_data_KDA[:,1],c=label,cmap = matplotlib.colors.ListedColormap(colors))\n",
    "    GONG = get_GONG_value(new_data_KDA, label, 'test')\n",
    "    plt.title('KDA: ' + str(round(GONG, 4)))\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.savefig('./result/' + fname + '/' + fname.split('.')[0] + '_KDA.png')\n",
    "    return GONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scikit-learn-compatible Kernel Discriminant Analysis.\n",
    "\n",
    "Used in\n",
    "David Diaz-Vico, Jose R. Dorronsoro\n",
    "\"Deep vs Kernel Fisher Discriminant Analysis\"\n",
    "\n",
    "Based on algorithm 5 in\n",
    "Zhihua Zhang, Guang Dai, Congfu Xu, Michael I. Jordan\n",
    "\"Regularized Discriminant Analysis, Ridge Regression and Beyond\"\n",
    "http://www.jmlr.org/papers/v11/zhang10b.html\n",
    "\n",
    ",author: David Diaz Vico\n",
    ",license: MIT\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.metrics.pairwise import (chi2_kernel, laplacian_kernel,\n",
    "                                      linear_kernel, polynomial_kernel,\n",
    "                                      rbf_kernel, sigmoid_kernel)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.validation import check_array, check_is_fitted, check_X_y\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "\n",
    "class KernelDiscriminantAnalysis(BaseEstimator, ClassifierMixin,\n",
    "                                 TransformerMixin):\n",
    "    \"\"\"Kernel Discriminant Analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lmb: float (>= 0.0), default=0.001\n",
    "         Regularization parameter\n",
    "    kernel: {\"chi2\", \"laplacian\", \"linear\", \"polynomial\", \"rbf\", \"sigmoid\"},\n",
    "            default='rbf'\n",
    "            Kernel.\n",
    "    degree: integer, default=3\n",
    "    gamma: float, default=None\n",
    "    coef0: integer, default=1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lmb=0.001, kernel='rbf', degree=3, gamma=None, coef0=1):\n",
    "        self.lmb = lmb\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "\n",
    "    def _kernel(self, X, Y=None):\n",
    "        \"\"\"Kernel\"\"\"\n",
    "        kernel = None\n",
    "        if self.kernel == 'chi2':\n",
    "            kernel = chi2_kernel(X, Y, gamma=self.gamma)\n",
    "        elif self.kernel == 'laplacian':\n",
    "            kernel = laplacian_kernel(X, Y, gamma=self.gamma)\n",
    "        elif self.kernel == 'linear':\n",
    "            kernel = linear_kernel(X, Y)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            kernel = polynomial_kernel(X, Y, degree=self.degree,\n",
    "                                       gamma=self.gamma, coef0=self.coef0)\n",
    "        elif self.kernel == 'rbf':\n",
    "            kernel = rbf_kernel(X, Y, gamma=self.gamma)\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            kernel = sigmoid_kernel(X, Y, gamma=self.gamma, coef0=self.coef0)\n",
    "        return kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit KFDA model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array of shape [n_samples, n_features]\n",
    "           Training set.\n",
    "        y: numpy array of shape [n_samples]\n",
    "           label values. Only works for 2 classes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        self._X = X\n",
    "        self._H = np.identity(n) - np.matmul(1. / n * np.ones(n), np.ones(n).T) #np.ones(n) , np.ones(n).T\n",
    "        self._E = OneHotEncoder().fit_transform(y.reshape(n, 1))\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        K = self._kernel(X)\n",
    "        C = np.matmul(np.matmul(self._H, K), self._H)\n",
    "        self._Delta = np.linalg.inv(C + self.lmb * np.identity(n))\n",
    "        A = np.matmul(csc_matrix.toarray(self._E.T), C)\n",
    "        B = np.matmul(self._Delta , csr_matrix.toarray(self._E))\n",
    "        self._Pi_12 = np.diag(np.sqrt(1.0 / counts))\n",
    "        P = np.matmul(self._Pi_12 , A)\n",
    "        Q = np.matmul(B , self._Pi_12)\n",
    "        R = np.matmul(P, Q)\n",
    "        V, self._Gamma, self._U = np.linalg.svd(R, full_matrices=False)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data with the trained KFDA model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array of shape [n_samples, n_features]\n",
    "           The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array-like, shape (n_samples, labels_shape)\n",
    "                Transformations for X.\n",
    "\n",
    "        \"\"\"\n",
    "        _K = self._kernel(X, self._X)\n",
    "        K = _K - np.mean(_K, axis=0)\n",
    "        C = np.matmul(self._H, K.T)\n",
    "        x = np.matmul(np.matmul(self._U, self._Pi_12), csc_matrix.toarray(self._E.T))\n",
    "        T = np.matmul(x,self._Delta)\n",
    "        Z = np.matmul(T, C)\n",
    "        return Z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(fname, n_cluster=3):\n",
    "    # get the data\n",
    "    data, label = read_data_from_file(fname)\n",
    "    # data, label = X, y\n",
    "    label = get_label_right(label)\n",
    "    label = np.array(list(map(int, label)))\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    print('+'*80)\n",
    "    # get the cluster\n",
    "    vec_dict, vecs = get_between_class_opt_vec(data, label)\n",
    "    id_pair, id_vecs = get_id_pair(vec_dict)\n",
    "    planes, pid_labels, pid_vids = get_all_planes(vecs, id_pair)\n",
    "    clusters = get_cluster(planes, pid_vids, id_vecs, n=n_cluster)\n",
    "    # get the projection\n",
    "    proj_labels, proj_set = get_proj_set_and_label(clusters, id_vecs, pid_labels, pid_vids)\n",
    "    draw_projection(proj_set, data, label, proj_labels)\n",
    "    score = get_weighted_GONG(data, label, proj_set, proj_labels)\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all('digits5_8.csv', n_cluster=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fname = 'world_9d.csv'\n",
    "# data, label = read_data_from_file(fname)\n",
    "# run_all(fname, 4)\n",
    "test_lda(data, label)\n",
    "test_pca(data, label)\n",
    "test_tsne(data, label)\n",
    "test_kda(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(fname, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2个类的文件都要踢出来\n",
    "path = r'.\\Data'\n",
    "dirs = os.listdir(path)\n",
    "for fname in dirs:\n",
    "    data, label = read_data_from_file(fname)\n",
    "        print(fname)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "731a7e1cffeec78bb69fd8b326238c05153c57f11389d9f58b7416e38442aa63"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('chapter21': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
